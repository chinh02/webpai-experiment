# Design2Code Experiments

This repository contains records of submissions to the Design2Code leaderboard, similar to [SWE-bench/experiments](https://github.com/SWE-bench/experiments).

## ğŸ“ Repository Structure

```
webpai-experiment/
â”œâ”€â”€ evaluation/                    # Model submissions organized by dataset
â”‚   â””â”€â”€ design2code/               # Design2Code benchmark submissions
â”‚       â”œâ”€â”€ 20251210_gpt4o_mark/
â”‚       â”‚   â”œâ”€â”€ evaluation.json    # Evaluation metrics
â”‚       â”‚   â”œâ”€â”€ metadata.yaml      # Submission metadata
â”‚       â”‚   â”œâ”€â”€ README.md          # Approach description
â”‚       â”‚   â””â”€â”€ results/           # Generated results
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ analysis/                      # Analysis scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ get_results.py             # Process individual submission
â”‚   â”œâ”€â”€ get_leaderboard.py         # Generate leaderboard data
â”‚   â”œâ”€â”€ process_all.py             # Batch process all submissions
â”‚   â”œâ”€â”€ compare.py                 # Compare submissions
â”‚   â””â”€â”€ api.py                     # API for website integration
â”‚
â”œâ”€â”€ data/                          # Generated leaderboard data
â”‚   â””â”€â”€ design2code-leaderboard.json
â”‚
â”œâ”€â”€ checklist.md                   # Submission requirements
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

## ğŸ” Viewing Results

### Get Results for a Submission

```bash
python -m analysis.get_results evaluation/design2code/<submission>

# Example
python -m analysis.get_results evaluation/design2code/20251210_gpt4o_mark
```

### Generate Leaderboard

```bash
python -m analysis.get_leaderboard

# With custom output
python -m analysis.get_leaderboard --output data/leaderboard.json
```

### Compare Submissions

```bash
python -m analysis.compare 20251210_gpt4o_mark 20251229_gpt4o_direct
```

### Process All Submissions

```bash
python -m analysis.process_all
```

## ğŸ† Leaderboard Participation

To submit to the Design2Code leaderboard:

1. **Fork** this repository
2. **Create** a new folder under `evaluation/design2code/` with format: `YYYYMMDD_modelname_method`
3. **Add** required files:
   - `evaluation.json` - Evaluation metrics
   - `metadata.yaml` - Submission metadata
   - `README.md` - Approach description
4. **Run** validation:
   ```bash
   python -m analysis.get_results evaluation/design2code/YOUR_SUBMISSION
   ```
5. **Create** a Pull Request

See [checklist.md](checklist.md) for detailed submission requirements.

## ğŸ“Š Metrics

Submissions are evaluated on:

| Metric | Description |
|--------|-------------|
| **CLIP** | Visual similarity using CLIP embeddings |
| **SSIM** | Structural similarity index |
| **Text Similarity** | Text content matching accuracy |
| **Position Accuracy** | Element positioning accuracy |
| **Image Reproduction** | Image element reproduction quality |

**Overall Score** = Weighted average (20% each)

## ğŸ”§ API Usage

For website integration:

```python
from analysis.api import get_leaderboard_data, get_submission_details

# Get leaderboard
leaderboard = get_leaderboard_data("design2code")

# Get submission details
details = get_submission_details("design2code", "20251210_gpt4o_mark")
```

## ğŸ“‹ Example Submission Structure

```
evaluation/design2code/20251210_gpt4o_mark/
â”œâ”€â”€ evaluation.json      # Required: Metrics data
â”œâ”€â”€ metadata.yaml        # Required: Model/method info
â”œâ”€â”€ README.md            # Required: Approach description
â”œâ”€â”€ trajs/               # Optional: Reasoning traces
â”‚   â”œâ”€â”€ instance_0.json
â”‚   â””â”€â”€ ...
â””â”€â”€ outputs/             # Optional: Generated code
    â”œâ”€â”€ instance_0.html
    â””â”€â”€ ...
```

## ğŸ“ Contact

For questions, please create an issue in this repository.

## âœï¸ Citation

If you use this repository, please cite:

```bibtex
@misc{design2code-experiments,
  title={Design2Code Experiments},
  year={2025},
  url={https://github.com/your-org/webpai-experiment}
}
```
